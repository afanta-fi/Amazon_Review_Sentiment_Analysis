{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Business-Understanding\" data-toc-modified-id=\"Business-Understanding-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Business Understanding</a></span></li><li><span><a href=\"#Data-Understanding\" data-toc-modified-id=\"Data-Understanding-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Understanding</a></span></li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Preparation</a></span></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Custom-Classes\" data-toc-modified-id=\"Custom-Classes-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Custom Classes</a></span></li><li><span><a href=\"#Shallow-Learning\" data-toc-modified-id=\"Shallow-Learning-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Shallow Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Naive Bayes</a></span></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Logistic Regression</a></span></li></ul></li><li><span><a href=\"#Gradient-Boosting\" data-toc-modified-id=\"Gradient-Boosting-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Gradient Boosting</a></span><ul class=\"toc-item\"><li><span><a href=\"#XGBoost\" data-toc-modified-id=\"XGBoost-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>XGBoost</a></span></li><li><span><a href=\"#CatBoost\" data-toc-modified-id=\"CatBoost-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>CatBoost</a></span></li></ul></li><li><span><a href=\"#Sequence-Models\" data-toc-modified-id=\"Sequence-Models-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Sequence Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gated-Recurrent-Unit\" data-toc-modified-id=\"Gated-Recurrent-Unit-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Gated Recurrent Unit</a></span></li><li><span><a href=\"#Long-Short-Term-Memory\" data-toc-modified-id=\"Long-Short-Term-Memory-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Long Short Term Memory</a></span></li></ul></li><li><span><a href=\"#Transformer-Models\" data-toc-modified-id=\"Transformer-Models-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Transformer Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bidirectional-Encoder-Representations-from-Transformers\" data-toc-modified-id=\"Bidirectional-Encoder-Representations-from-Transformers-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>Bidirectional Encoder Representations from Transformers</a></span></li><li><span><a href=\"#Hugging-Face\" data-toc-modified-id=\"Hugging-Face-4.5.2\"><span class=\"toc-item-num\">4.5.2&nbsp;&nbsp;</span>Hugging Face</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Models'-Performances\" data-toc-modified-id=\"Models'-Performances-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Models' Performances</a></span></li><li><span><a href=\"#Final-Model\" data-toc-modified-id=\"Final-Model-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Final Model</a></span></li><li><span><a href=\"#Model-Interpretation\" data-toc-modified-id=\"Model-Interpretation-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Model Interpretation</a></span></li><li><span><a href=\"#API-Development\" data-toc-modified-id=\"API-Development-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>API Development</a></span></li><li><span><a href=\"#Web-Deployment\" data-toc-modified-id=\"Web-Deployment-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Web Deployment</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Next-Steps\" data-toc-modified-id=\"Next-Steps-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Next Steps</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyenchant \n",
    "# ! pip install autocorrect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import bz2\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "import re \n",
    "from autocorrect import Speller\n",
    "from enchant import request_dict\n",
    "from enchant.checker import SpellChecker\n",
    "from enchant.tokenize import EmailFilter, URLFilter\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib.concurrent import process_map \n",
    "from multiprocessing import Pool\n",
    "from time import time\n",
    "from os.path import exists\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,\\\n",
    "HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\zeaps\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zeaps\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zeaps\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train data: 100%|██████████| 3600000/3600000 [00:03<00:00, 1121784.10it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_data(b2z_file_loc):\n",
    "    file = bz2.BZ2File(b2z_file_loc)\n",
    "    t = tqdm(file.readlines())\n",
    "    t.set_description('Loading '+b2z_file_loc.split('/')[-1].split('.')[0]+' data')\n",
    "    return [x.decode('utf-8') for x in t]\n",
    "        \n",
    "train_file_lines = load_data('data/train.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\\n',\n",
       " \"__label__2 The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\\n\",\n",
       " '__label__2 Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you\\'ve played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time\\'s Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer\\'s work (I haven\\'t heard the Xenogears soundtrack, so I can\\'t say for sure), and even if you\\'ve never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.\\n',\n",
       " \"__label__2 Excellent Soundtrack: I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it's truly relaxing and peaceful.On disk one. my favorites are Scars Of Time, Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Lost Fragment, and Drowned Valley.Disk Two: The Draggons, Galdorb - Home, Chronomantique, Prisoners of Fate, Gale, and my girlfriend likes ZelbessDisk Three: The best of the three. Garden Of God, Chronopolis, Fates, Jellyfish sea, Burning Orphange, Dragon's Prayer, Tower Of Stars, Dragon God, and Radical Dreamers - Unstealable Jewel.Overall, this is a excellent soundtrack and should be brought by those that like video game music.Xander Cross\\n\",\n",
       " \"__label__2 Remember, Pull Your Jaw Off The Floor After Hearing it: If you've played the game, you know how divine the music is! Every single song tells a story of the game, it's that good! The greatest songs are without a doubt, Chrono Cross: Time's Scar, Magical Dreamers: The Wind, The Stars, and the Sea and Radical Dreamers: Unstolen Jewel. (Translation varies) This music is perfect if you ask me, the best it can be. Yasunori Mitsuda just poured his heart on and wrote it down on paper.\\n\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"__label__1 The Worst!: A complete waste of time. Typographical errors, poor grammar, and a totally pathetic plot add up to absolutely nothing. I'm embarrassed for this author and very disappointed I actually paid for this book.\\n\",\n",
       " '__label__2 Great book: This was a great book,I just could not put it down,and could not read it fast enough. Boy what a book the twist and turns in this just keeps you guessing and wanting to know what is going to happen next. This book makes you fall in love and can heat you up,it can also make you so angery. this book can make you go throu several of your emotions. This is a quick read romance. It is something that you will want to end your day off with if you read at night.\\n',\n",
       " '__label__2 Great Read: I thought this book was brilliant, but yet realistic. It showed me that to error is human. I loved the fact that this writer showed the loving side of God and not the revengeful side of him. I loved how it twisted and turned and I could not put it down. I also loved The glass castle.\\n',\n",
       " \"__label__1 Oh please: I guess you have to be a romance novel lover for this one, and not a very discerning one. All others beware! It is absolute drivel. I figured I was in trouble when a typo is prominently featured on the back cover, but the first page of the book removed all doubt. Wait - maybe I'm missing the point. A quick re-read of the beginning now makes it clear. This has to be an intentional churning of over-heated prose for satiric purposes. Phew, so glad I didn't waste $10.95 after all.\\n\",\n",
       " '__label__1 Awful beyond belief!: I feel I have to write to keep others from wasting their money. This book seems to have been written by a 7th grader with poor grammatical skills for her age! As another reviewer points out, there is a misspelling on the cover, and I believe there is at least one per chapter. For example, it was mentioned twice that she had a \"lean\" on her house. I was so distracted by the poor writing and weak plot, that I decided to read with a pencil in hand to mark all of the horrible grammar and spelling. Please don\\'t waste your money. I too, believe that the good reviews must have been written by the author\\'s relatives. I will not put much faith in the reviews from now on!\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_lines[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkr = SpellChecker(\"en_US\",filters=[EmailFilter,URLFilter])\n",
    "spell = Speller(lang='en')\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "sw = stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def data_cleaner(data):\n",
    "    splits = data.split('__label__')[-1]\n",
    "    labels = int(splits[0])-1    \n",
    "    statement = decontracted(splits[1:].lower())\n",
    "    statement = spell(statement)\n",
    "    chkr.set_text(statement)\n",
    "    for err in chkr:\n",
    "        try:            \n",
    "            statement = statement.replace(err.word,request_dict(err.word)[0])\n",
    "        except:\n",
    "            continue\n",
    "    statement = tokenizer.tokenize(statement)        \n",
    "    statement = [w for w in statement if w not in sw]\n",
    "    statement_tagged = [(token[0], get_wordnet_pos(token[1]))\n",
    "                        for token in pos_tag(statement)]\n",
    "    statement_lemmed = [lemmatizer.lemmatize(token[0], token[1]) for token in statement_tagged]\n",
    "    return statement_lemmed, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tune',\n",
       " 'even',\n",
       " 'non',\n",
       " 'gamer',\n",
       " 'sound',\n",
       " 'track',\n",
       " 'beautiful',\n",
       " 'paint',\n",
       " 'scenery',\n",
       " 'mind',\n",
       " 'well',\n",
       " 'would',\n",
       " 'recommend',\n",
       " 'even',\n",
       " 'people',\n",
       " 'hate',\n",
       " 'vid',\n",
       " 'game',\n",
       " 'music',\n",
       " 'play',\n",
       " 'game',\n",
       " 'chrono',\n",
       " 'cross',\n",
       " 'game',\n",
       " 'ever',\n",
       " 'play',\n",
       " 'best',\n",
       " 'music',\n",
       " 'back',\n",
       " 'away',\n",
       " 'crude',\n",
       " 'keyboardist',\n",
       " 'take',\n",
       " 'fresh',\n",
       " 'step',\n",
       " 'rate',\n",
       " 'guitar',\n",
       " 'soulful',\n",
       " 'orchestra',\n",
       " 'would',\n",
       " 'impress',\n",
       " 'anyone',\n",
       " 'care',\n",
       " 'listen']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement, labels = data_cleaner(train_file_lines[0])\n",
    "statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(file_lines, attrib):\n",
    "    currtime = time()\n",
    "    results = process_map(data_cleaner, file_lines, \n",
    "                          max_workers=Pool()._processes, \n",
    "                          chunksize=Pool()._processes)\n",
    "    statements = []\n",
    "    labels = []\n",
    "    for result in results:\n",
    "        statements.append(result[0])\n",
    "        labels.append(result[-1])\n",
    "    df = pd.DataFrame(columns=['statements','labels'])\n",
    "    df.statements = statements\n",
    "    df.labels = labels\n",
    "    df.to_csv(r'data/'+attrib+'.zip', index=False, compression='gzip')\n",
    "    print('Parallel: time elapsed:', time() - currtime)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists('data/train.zip') and exists('data/test.zip'):\n",
    "#     del train_file_lines\n",
    "    train_df = pd.read_csv('data/train.zip', compression='gzip')\n",
    "    def str_cleaner(line):\n",
    "        return line[1:-1].replace(\"'\",'').replace(' ','').replace(',',' ')\n",
    "    train_df.statements = train_df.statements.apply(str_cleaner)\n",
    "else:\n",
    "    train_df = save_to_file(train_file_lines, 'train') \n",
    "    del train_file_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statements</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tune even non gamer sound track beautiful pain...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best soundtrack ever anything reading lot revi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amaze soundtrack favorite music time hand inte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excellent soundtrack truly like soundtrack enj...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remember pull jaw floor hear played game know ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          statements  labels\n",
       "0  tune even non gamer sound track beautiful pain...       1\n",
       "1  best soundtrack ever anything reading lot revi...       1\n",
       "2  amaze soundtrack favorite music time hand inte...       1\n",
       "3  excellent soundtrack truly like soundtrack enj...       1\n",
       "4  remember pull jaw floor hear played game know ...       1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.5\n",
       "0    0.5\n",
       "Name: labels, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.labels.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_vec = TfidfVectorizer()\n",
    "# X = tf_vec.fit_transform(train_df.statements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Memory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Encoder Representations from Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models' Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Development "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Deployment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
